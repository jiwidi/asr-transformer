amp_level: '02'
batch_size: 4
data_root: /mnt/kingston/datasets/
data_test:
- test-clean
data_train:
- train-clean-100
- train-clean-360
- train-other-500
decoder: null
early_stop_metric: wer
early_stop_patience: 3
encoder: null
epochs: 100
experiment_name: DeepSpeech
learning_rate: 0.0005
limit_train_batches: 0.01
logs_path: runs/
num_workers: 4
precision: 32
resume_from_checkpoint: null
